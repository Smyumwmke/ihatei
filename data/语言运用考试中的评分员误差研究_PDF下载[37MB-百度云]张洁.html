语言运用考试中的评分员误差研究 PDF下载 张洁 百度云 电子书 下载 电子书下载
PDF电子书下载不求人，看这篇文章就够了→ http://www.chendianrong.com/pdf#730815743
PDF电子书下载不求人，看这篇文章就够了→ http://www.chendianrong.com/pdf#730815743
<p>书名:语言运用考试中的评分员误差研究</p><p>作者:张洁</p><p>页数:174</p><p>定价:¥35.0</p><p>出版社:浙江大学出版社</p><p>出版日期:2016-04-01</p><p>ISBN:9787308157438</p><p><h2>本书特色</h2></p>[<p>
张洁博士的专著针对语言运用考试中的评分误差问题，主要探讨评分员误差对考试信效度的影响、误差的主要类型及造成误差的可能认知因素，从不同视角全面讨论了语言运用测试中的评分员误差问题。《语言运用考试中的评分员误差研究(英文版)》不仅系统地梳理了语言测试领域中评分误差的相关研究，介绍了该研究方向经常使用的定性定量研究方法，还通过两个实证研究翔实、具体地说明了如何运用不同的研究方法分析、测量和探究主观评分可能存在的误差及原因，在理论和实践层面都有较强的指导意义。书中包括的两个实证研究分别是她在硕士和博士阶段研究的主要内容，前者对应定量统计研究范式，运用多层面rasch模型对四、六级口语考试的分数差异来源进行了系统研究；后者对应定性的以过程为导向的研究范式，对四级作文评分中评分员认知过程对评分准确度的影响进行了探讨。两个研究的主要发现对于大规模语言运用测试中评分误差的控制、测量以及更加有效地开展评分员培训和评分标准修订都具有十分重要的借鉴意义。
                                        </p>]<p><h2>目录</h2></p>
    chapter 1 introduction  1.1  rationales for studying rater variability  1.2  status quo of studies on rater variability  1.3  an overview of this book  1.4  definition of key termschapter 2 literature review: studies on rater variability in language performance assessment  2.1  rater variability in language performance assessment  2.2  exploring rater variability using statistical analysis    2.2.1  introduction    2.2.2  rater reliability in classical test theory    2.2.3  rater facet as variance component in generalizability theory ""    2.2.4  rater calibration in many-facet rasch model    2.2.5  summary  2.3  process-oriented approach to investigating rater variability    2.3.1  raters' decision-making: the "black box" behind the final ratings"    2.3.2  indirect evidence    2.3.3  direct investigation of rating process: insights from verbal  protocols  2.4  factors accounting for rater variability    2.4.1  external factors    2.4.2  internal factors    2.4.3  situational factors  2.5  a framework for comparison between rater groups  2.6  summarychapter 3 study 1: investigating the scoring reliability of cet-set using many-facet rasch model  3.1  issues in second language speaking assessment  3.2  challenges in test validation  3.3  the context of the study  3.4  objectives of the study  3.5  methods    3.5.1  data    3.5.2  instrument (mfrm)  3.6  data analyses and findings    3.6.1  facet map    3.6.2  candidates    3.6.3  tasks    3.6.4  items    3.6.5  rating scales    3.6.6  raters    3.6.7  bias analysis  3.7  conclusions  3.8  implications  3.9  further research efforts to be madechapter 4 study 2: exploring how raters' cognitive and meta-cognitive strategies influence rating accuracy in essay scoring  4.1  subjective scoring: a matter of reliability or validity?  4.2  exploring rating process: looking into rater variability  4.3  rater cognition studies in writing assessment  4.4  methodology    4.4.1  the context of the study    4.4.2  participants    4.4.3  materials    4.4.4  data collection    4.4.5  data analysis  4.5  results and discussion     4.5.1  general patterns of differences in broad categories     4.5.2  in-depth investigation of differences in the major sub-categories  4.6  summary and further discussion  4.7  conclusionchapter 5 conclusions  5.1  summary of findings  5.2  comparison of the two studies  5.3  limitations  5.4  further research efforts to be madeappendix i  cet-set rating scaleappendix ii  cet4 rating rubrics for the writing taskappendix iii  the writing task of the dec. 2006 administration of cet4 and range findersappendix iv sample essaysappendix v  instructions and training tasks for think-aloud sessionappendix vi sample transcripts of raters' thinking aloudappendix vii coding protocols for think-aloud verbal reportsappendix viii the coding scheme for raters' cognitive and meta-cognitive strategiesreferencesindex
